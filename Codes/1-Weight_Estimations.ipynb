{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical and Scientific Computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import math\n",
    "from numba import jit\n",
    "\n",
    "\n",
    "from scipy import special\n",
    "from scipy.integrate import quad\n",
    "from scipy import integrate\n",
    "from scipy import stats\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Plotting and Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator)\n",
    "import matplotlib as mpl\n",
    "from matplotlib import ticker, cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "from matplotlib import rc\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "import pyregion\n",
    "\n",
    "# File Handling\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "# Astronomical\n",
    "\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "from astropy.cosmology import Planck15\n",
    "import astropy.units as u\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astroML.stats import binned_statistic_2d\n",
    "\n",
    "\n",
    "# Other\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = 3e5\n",
    "H_0 = 70\n",
    "Omega_l = 0.7\n",
    "Omega_m = 0.3\n",
    "\n",
    "lim_deltaz = 2\n",
    "\n",
    "cosmo = FlatLambdaCDM(H0 = H_0, Om0 = Omega_m)\n",
    "\n",
    "###########################################################################\n",
    "############################ LSS functions ################################\n",
    "###########################################################################\n",
    "\n",
    "def M_lim(z):\n",
    "    \"\"\"Fitting function for the mass completeness limit Weaver et. al 2022\"\"\"\n",
    "    return np.log10(-1.51e6 * (1+z) + 6.8e7 * (1+z)**2)\n",
    "\n",
    "def M_lim_ks(z):\n",
    "    \"\"\"Fitting function for mass completeness limit (on K_s) Weaver et al 2022\"\"\"\n",
    "    return np.log10(-3.55e8 * (1+z) + 2.7e8 * (1+z)**2)\n",
    "\n",
    "def slice_width(z):\n",
    "    \"\"\"Calculate the width of each redshift slice of size _physical_width_ (Mpc h^-1)\"\"\"\n",
    "    return physical_width * 100 / c0 * np.sqrt(Omega_m * (1+z)**3 + Omega_l)\n",
    "\n",
    "\n",
    "def redshift_bins(zmin, zmax):\n",
    "    \"\"\"returns the slice centers and widths, given a physical length in (Mpc h^-1) \"\"\"\n",
    "    centers = []\n",
    "    centers.append(zmin + 0.5 * slice_width(zmin))\n",
    "\n",
    "    i = 0\n",
    "    while (centers[i] + slice_width(centers[i]) < zmax ):\n",
    "        centers.append(centers[i] + slice_width(centers[i]))\n",
    "        i += 1\n",
    "\n",
    "    centers = np.array(centers)\n",
    "\n",
    "    \"redshift edges\"\n",
    "    edges = np.zeros((len(centers), 2))\n",
    "\n",
    "    for i in range(0, len(centers)):\n",
    "        edges[i, 0] = centers[i] - slice_width(centers[i]) / 2\n",
    "        edges[i, 1] = centers[i] + slice_width(centers[i]) / 2\n",
    "\n",
    "    return (centers, edges)\n",
    "\n",
    "\n",
    "def cartesian_from_polar(phi, theta):\n",
    "    \"\"\" \n",
    "    phi, theta : float or numpy.array\n",
    "        azimuthal and polar angle in radians.\n",
    "    Returns\n",
    "    -------\n",
    "    nhat : numpy.array\n",
    "        unit vector(s) in direction (phi, theta).\n",
    "    \"\"\"\n",
    "    x = np.sin(theta) * np.cos(phi)\n",
    "    y = np.sin(theta) * np.sin(phi)\n",
    "    z = np.cos(theta)\n",
    "    return np.array([x, y, z])\n",
    "\n",
    "def cos_dist(alpha, delta, alpha0, delta0):\n",
    "    \"\"\" gets all angles in [deg]\"\"\"\n",
    "    phi = alpha * np.pi / 180\n",
    "    theta = np.pi / 2 - delta * np.pi / 180\n",
    "    phi0 = alpha0 * np.pi / 180\n",
    "    theta0 = np.pi / 2 - delta0 * np.pi / 180\n",
    "    \n",
    "    x = cartesian_from_polar(phi, theta)\n",
    "    x0 = cartesian_from_polar(phi0, theta0)\n",
    "    cosdist = np.tensordot(x, x0, axes=[[0], [0]])\n",
    "    return np.clip(cosdist, 0, 1)\n",
    "\n",
    "def logsinh(x):\n",
    "    if np.any(x < 0):\n",
    "        raise ValueError(\"logsinh only valid for positive arguments\")\n",
    "    return x + np.log(1-np.exp(-2*x)) - np.log(2)\n",
    "\n",
    "def Log_K(alpha, delta, alpha0, delta0, kappa):\n",
    "    norm = -np.log(4 * np.pi / kappa) - logsinh(kappa)\n",
    "    return norm + cos_dist(alpha, delta, alpha0, delta0) * kappa\n",
    "\n",
    "def σ_k(X0, b, points):\n",
    "    kappa = 1 / (b * np.pi / 180)**2\n",
    "    X0_x = points[X0, 0]\n",
    "    X0_y = points[X0, 1]\n",
    "    rem = np.delete(points, X0, axis = 0)\n",
    "    arr = rem[:, 2] * np.exp(Log_K(rem[:, 0], rem[:, 1], X0_x, X0_y, kappa))\n",
    "    return np.sum(arr)\n",
    "    \n",
    "def LCV(b, points):\n",
    "    N = len(points)\n",
    "    arr1 = [np.log(σ_k(i, b, points)) for i in range(0, len(points))]\n",
    "    return (1 / N) * np.sum(arr1)\n",
    "\n",
    "def σ_k_gaussian(X0, b, points):\n",
    "    X0_x = points[X0, 0]\n",
    "    X0_y = points[X0, 1]\n",
    "    rem = np.delete(points, X0, axis = 0)\n",
    "\n",
    "    Cosdists = cos_dist(rem[:, 0], rem[:, 1], X0_x, X0_y)\n",
    "    arr = rem[:, 2] * norm.pdf(np.arccos(Cosdists[:]), loc = 0, scale = b * np.pi / 180)\n",
    "    return np.sum(arr)\n",
    "\n",
    "def σ(alpha, delta, b_i, points):\n",
    "    kappa = 1 / (b_i * np.pi / 180)**2\n",
    "    arr2 = points[:, 2] * np.exp(Log_K(points[:, 0], points[:, 1], alpha, delta, kappa))\n",
    "    return np.sum(arr2)\n",
    "\n",
    "def Adaptive_b(b, points):\n",
    "    g_i = np.array([np.log(points[i, 4] * σ(points[i, 0], points[i, 1], b, points)) for i in range(0, len(points))])\n",
    "    log_g = 1 / len(points) * np.sum(g_i)\n",
    "    b_i = np.array([(b * (points[i, 4] * σ(points[i, 0], points[i, 1], b, points) / np.exp(log_g))** -0.5) for i in tqdm(range(0, len(points)))])\n",
    "    return b_i\n",
    "\n",
    "def divider_NUV(rj):\n",
    "    return (3*rj+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(work_path='.'):\n",
    "    '''\n",
    "    Set up all of the necessary directories\n",
    "    '''\n",
    "    for subdir in ('inputs', 'outputs', 'bin', \n",
    "                   'outputs/plots', 'outputs/weights', 'outputs/density'):\n",
    "        path = os.path.join(work_path, subdir)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "            print(f'Built directory: {os.path.abspath(path)}')\n",
    "    \n",
    "    outputs_dir = os.path.join(work_path, 'outputs')\n",
    "    plots_dir = os.path.join(work_path, 'outputs', 'plots')\n",
    "    inputs_dir = os.path.join(work_path, 'inputs')\n",
    "    weight_dir = os.path.join(work_path, 'outputs', 'weights')\n",
    "    density_dir = os.path.join(work_path, 'outputs', 'density') \n",
    "    return outputs_dir, plots_dir, inputs_dir, weight_dir, density_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dir = \"where you want to set up the catalog directories\"\n",
    "\n",
    "outputs_dir, plots_dir, inputs_dir, weights_dir, density_dir = setup(work_path=cat_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_min, z_max = 0.4, 9.5\n",
    "\n",
    "physical_width = 35 # h^-1 Mpc\n",
    "\n",
    "slice_centers, z_edges = redshift_bins(z_min, z_max)\n",
    "\n",
    "z_width = z_edges[:, 1] - z_edges[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = \"path to your data file\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assuming Gaussian Photo z PDF\n",
    "weight of galaxy \"g\" in slice \"s\"\n",
    "$w[\\text{g}][\\text{s}]=\\int_{b}^{c} \\frac{1}{\\sqrt{2 \\pi} \\sigma}\\exp^{\\frac{-(x- \\mu)^{2}}{2\\sigma^{2}}}dx$\n",
    "    \n",
    "$w[\\text{g}][\\text{s}]=\\frac{1}{2}[erf(a(c-\\mu))-erf(a(b-\\mu))]$\n",
    "    \n",
    "$a=\\frac{1}{\\sqrt{2}\\sigma}$ \n",
    "\n",
    "zPDFs are assumed to be Gaussian with mean: zPDF and std= (zPDF_u68 - zPDF_l68) / 2\n",
    "\n",
    "weights[i][j]: weight of ith galaxy in the jth slice <br>\n",
    "first column is Ids, then weights in n columns (redshift slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.05 # threshold for low weight cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Flagged Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Prepare Gaussian parameters ---\n",
    "mu = Data['zPDF'].to_numpy()\n",
    "delta_z = (Data['zPDF_u68'] - Data['zPDF_l68']).to_numpy()\n",
    "sigma = delta_z / 2\n",
    "z_edges_array = np.array(z_edges)\n",
    "n_slices = len(z_edges_array)\n",
    "\n",
    "# --- Step 2: Compute Gaussian weights---\n",
    "weights_block = 0.5 * (\n",
    "    special.erf((z_edges_array[:, 1] - mu[:, None]) / (np.sqrt(2) * sigma[:, None])) -\n",
    "    special.erf((z_edges_array[:, 0] - mu[:, None]) / (np.sqrt(2) * sigma[:, None]))\n",
    ")\n",
    "\n",
    "# --- Step 3: Normalize weights ---\n",
    "row_sums = np.sum(weights_block, axis=1, keepdims=True)\n",
    "row_sums[row_sums == 0] = 1  # prevent divide-by-zero\n",
    "weights_block = weights_block / row_sums\n",
    "\n",
    "# --- Step 4: Add IDs to form full weights matrix ---\n",
    "weights = np.zeros((len(Data), n_slices + 1))\n",
    "weights[:, 0] = Data['id'].to_numpy()\n",
    "weights[:, 1:] = weights_block\n",
    "\n",
    "# --- Step 5: Apply threshold to find significant contributions ---\n",
    "mask = weights_block > threshold\n",
    "ind = np.column_stack(np.nonzero(mask))  # shape: (n_selected, 2)\n",
    "ind = ind[np.argsort(ind[:, 0])]  # sort by galaxy index\n",
    "\n",
    "# --- Step 6: Group galaxies by slice ---\n",
    "gals_bin = [np.unique(ind[ind[:, 1] == i, 0]) for i in range(n_slices)]\n",
    "\n",
    "# --- Step 7: Re-normalize using only selected slice weights ---\n",
    "W = weights_block.copy()\n",
    "selected_weights = W[ind[:, 0], ind[:, 1]]\n",
    "unique_ids, inverse_idx = np.unique(ind[:, 0], return_inverse=True)\n",
    "sums = np.bincount(inverse_idx, weights=selected_weights)\n",
    "sums[sums == 0] = 1\n",
    "W[ind[:, 0], ind[:, 1]] = selected_weights / sums[inverse_idx]\n",
    "\n",
    "# Compute count and median in redshift slices\n",
    "count_in_zslice = np.array([\n",
    "    np.sum((Data['zPDF'] > z_edges[i, 0]) & (Data['zPDF'] < z_edges[i, 1])) \n",
    "    for i in range(len(slice_centers))\n",
    "])\n",
    "\n",
    "delta_z_median = np.array([\n",
    "    np.median((Data.loc[(Data['zPDF'] > z_edges[i, 0]) & \n",
    "                        (Data['zPDF'] < z_edges[i, 1]), 'zPDF_u68'] - \n",
    "               Data.loc[(Data['zPDF'] > z_edges[i, 0]) & \n",
    "                        (Data['zPDF'] < z_edges[i, 1]), 'zPDF_l68']) / 2)\n",
    "    for i in range(len(slice_centers))\n",
    "])\n",
    "\n",
    "# Compute normalized dz / (1 + z) median\n",
    "normalized_delta_z_median = np.array([\n",
    "    np.median((Data.loc[(Data['zPDF'] > z_edges[i, 0]) & (Data['zPDF'] < z_edges[i, 1]), 'zPDF_u68'] - \n",
    "               Data.loc[(Data['zPDF'] > z_edges[i, 0]) & (Data['zPDF'] < z_edges[i, 1]), 'zPDF_l68']) / \n",
    "              (2 * (1 + Data.loc[(Data['zPDF'] > z_edges[i, 0]) & (Data['zPDF'] < z_edges[i, 1]), 'zPDF'])))\n",
    "    if np.any((Data['zPDF'] > z_edges[i, 0]) & (Data['zPDF'] < z_edges[i, 1])) else np.nan\n",
    "    for i in range(len(slice_centers))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with updated values\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(10, 8), gridspec_kw={'height_ratios': [1, 0.8]})\n",
    "\n",
    "# Top bar plot dark olive\n",
    "ax[0].bar(slice_centers, count_in_zslice, width=z_width, color='indigo')\n",
    "\n",
    "ax[0].set_xlim(z_min, z_max)\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_ylabel('Number of galaxies', fontsize=16)\n",
    "ax[0].yaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "ax[0].tick_params(axis='y', labelsize=12)\n",
    "ax[0].set_yscale('log')\n",
    "\n",
    "\n",
    "# Bottom hexbin plot\n",
    "hb = ax[1].hexbin(Data['zPDF'], sigma / (1 + Data['zPDF']), gridsize=20, cmap='viridis', bins='log')\n",
    "step = 4\n",
    "ax[1].plot(slice_centers[::step], normalized_delta_z_median[::step],\n",
    "           color='red', linestyle='dashed', linewidth=2)\n",
    "\n",
    "ax[1].set_xlabel('z', fontsize=16)\n",
    "ax[1].set_ylabel(r'$\\sigma_z / (1 + z)$', fontsize=16)\n",
    "ax[1].set_xlim(z_min, z_max)\n",
    "ax[1].xaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "# Add colorbar\n",
    "cb = fig.colorbar(hb, ax=ax[1], orientation='horizontal', pad=0.3, fraction=0.09)\n",
    "cb.set_label('counts', fontsize=16)\n",
    "cb.ax.tick_params(labelsize=12)\n",
    "\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "# Adjust spacing and save the plot\n",
    "plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.1)\n",
    "plt.savefig(os.path.join(plots_dir, f'histogram_th{threshold}_lengh{physical_width}_flagged.png'),\n",
    "            format='png', dpi=600, bbox_inches='tight')\n",
    "plt.savefig(os.path.join(plots_dir, f'histogram_th{threshold}_lengh{physical_width}_flagged.pdf'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(weights_dir, f'weights_unthresholded_normalized_thresh{threshold}_lengh{physical_width}.npy'), weights)\n",
    "np.save(os.path.join(weights_dir, f'weightsBlock_unthresholded_normalized_thresh{threshold}_lengh{physical_width}.npy'), weights_block)\n",
    "np.save(os.path.join(weights_dir, f'weightsBlock_thresh{threshold}_normalized_lengh{physical_width}.npy'), W)\n",
    "np.save(os.path.join(weights_dir, f'normalized_delta_z_median_thresh{threshold}_lengh{physical_width}.npy'), normalized_delta_z_median)\n",
    "np.save(os.path.join(weights_dir, f'delta_z_median_thresh{threshold}_lengh{physical_width}.npy'), delta_z_median)\n",
    "np.save(os.path.join(weights_dir, f'count_in_zslice_thresh{threshold}_lengh{physical_width}.npy'), count_in_zslice)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
